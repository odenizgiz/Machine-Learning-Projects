{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I adapt the Adaline implementation from the book [Python Machine Learning by Sebastian Raschka](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning) to the Logistic Regression for the case of 2 class labels. \n",
    "\n",
    "<p><p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "class LogisticRegressionFS(object): \n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\" initialize weights \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        \"\"\" update the weights after each iteration \"\"\"\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.activation(X)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            \n",
    "            \"\"\" make a prediction and calculate the cost function \"\"\"\n",
    "            prediction = self.predict(X)\n",
    "            log_act = np.log(self.activation(X))\n",
    "            log_act2 = np.log(1 - self.activation(X))\n",
    "            cost = (- prediction * log_act - (1 - prediction) * log_act2).sum()          \n",
    "            self.cost_.append(cost)\n",
    "        return self \n",
    "             \n",
    "            \n",
    "    def net_input(self, X):\n",
    "        \"\"\" Calculate the net input \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "            \n",
    "        \n",
    "    def activation(self, X):\n",
    "        \"\"\" Activation Function \"\"\"\n",
    "        z = self.net_input(X)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Making the prediction \"\"\" \n",
    "        return np.where(self.activation(X) >= 0.5, 1, 0)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        \"\"\" The probabilities of each entry in the data belonging to either class\"\"\"\n",
    "        return [self.activation(X), 1 - self.activation(X)]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predicting Class Labels for the Iris Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only pick the data entries that belong to the first two classes (which are the first 100 entries), and ignore the third class.    \n",
    "And we will consider only two features, instead of four: petal length and sepal length.\n",
    "\n",
    "\n",
    "<p><p/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:100, [2, 3]]\n",
    "y = iris.target[:100]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature scaling: standardize the features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegressionFS at 0x10ad7b208>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the Logistic Regression classifier to the training data\n",
    "logistic = LogisticRegressionFS(eta=0.01, n_iter=15)\n",
    "logistic.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br/> \n",
    "\n",
    "\n",
    "Now we can predict the labels for the test data, and assign numbers 1 and 0 to correct and incorrect predictions respectively. \n",
    "\n",
    "<p><p/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_predictions = np.where(y_test == logistic.predict(X_test_std), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of incorrect predictions:\n",
    "(my_predictions == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of correct predictions\n",
    "(my_predictions == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0906421511691161, 0.90935784883088389]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# probability that the first data in the test dataset belongs to classes 1 and 0 respectively:\n",
    "print (logistic.predict_prob(X_test_std[0]))\n",
    "# indeed this entry is in class 0:\n",
    "print (y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
